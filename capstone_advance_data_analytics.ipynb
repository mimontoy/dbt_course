{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy+8mYb2jz80uula6BNDze",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mimontoy/dbt_course/blob/main/capstone_advance_data_analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7Qhiafqw9R2",
        "outputId": "5677d034-2d78-4fc9-a3cd-64e008f78433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset here"
      ],
      "metadata": {
        "id": "-KVwdChsysF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "### YOUR CODE HERE ### \n",
        "\n",
        "# For data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For data visualization\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For displaying all of the columns in dataframes\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# For data modeling\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost import plot_importance\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# For metrics and helpful functions\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score,\\\n",
        "f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# For saving models\n",
        "import pickle"
      ],
      "metadata": {
        "id": "bgfI3hjvxECz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS CELL TO IMPORT YOUR DATA. \n",
        "\n",
        "# Load dataset into a dataframe\n",
        "### YOUR CODE HERE ### \n",
        "# Please find the dataset path\n",
        "df0 = pd.read_csv(\"HR_capstone_dataset.csv\")\n",
        "\n",
        "# other option adding index_col\n",
        "#data = pd.read_csv(\"nba-players.csv\", index_col=0)\n",
        "\n",
        "# Display first few rows of the dataframe\n",
        "### YOUR CODE HERE ### \n",
        "df0.head()\n",
        "# Display number of rows, number of columns.\n",
        "### YOUR CODE HERE ###\n",
        "df0.shape"
      ],
      "metadata": {
        "id": "P0s5ksmXxfwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather basic information about the data\n",
        "### YOUR CODE HERE ###\n",
        "df0.info()"
      ],
      "metadata": {
        "id": "Th4_iOCOyC3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather descriptive statistics about the data\n",
        "### YOUR CODE HERE ### \n",
        "df0.describe()"
      ],
      "metadata": {
        "id": "iC7YuZVhyXKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display all column names\n",
        "### YOUR CODE HERE ### \n",
        "df0.columns"
      ],
      "metadata": {
        "id": "1RCq5CcfzJ3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename columns as needed\n",
        "### YOUR CODE HERE ### \n",
        "df0 = df0.rename(columns={'Work_accident': 'work_accident',\n",
        "                          'average_montly_hours': 'average_monthly_hours',\n",
        "                          'time_spend_company': 'tenure',\n",
        "                          'Department': 'department'})\n",
        "\n",
        "# Display all column names after the update\n",
        "### YOUR CODE HERE ### \n",
        "df0.columns\n",
        "\n",
        "# Find the count of each class type.\n",
        "df0['department'].value_counts(dropna = False)\n",
        "\n",
        "# Display percentage (%) of values for each class (1, 0) represented in the target column of this dataset.\n",
        "df0[\"work_accident\"].value_counts(normalize=True)*100\n",
        "\n",
        "# Select the columns to proceed with and save the DataFrame in new variable `selected_df0`.\n",
        "# Include the target column, `target_5yrs`.\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "#selected_df0 = df0[[\"col1\", \"col2\", \"col3\", \"coln\"]]\n",
        "\n",
        "# Display the first few rows.\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "#selected_df0.head()\n",
        "\n",
        "# Extract two features that would help predict target_5yrs.\n",
        "# Create a new variable named `extracted_data`.\n",
        "\n",
        "### YOUR CODE HERE ### \n",
        "\n",
        "# Make a copy of `selected_data` \n",
        "#extracted_df0 = selected_df0.copy()\n",
        "\n",
        "# Add a new column named `total_points`; \n",
        "# Calculate total points earned by multiplying the number of games played by the average number of points earned per game\n",
        "#extracted_data[\"total_points\"] = extracted_data[\"gp\"] * extracted_data[\"pts\"]\n",
        "\n",
        "# Add a new column named `efficiency`. Calculate efficiency by dividing the total points earned by the total number of minutes played, which yields points per minute\n",
        "#extracted_data[\"efficiency\"] = extracted_data[\"total_points\"] / extracted_data[\"min\"]\n",
        "\n",
        "# Display the first few rows of `extracted_data` to confirm that the new columns were added.\n",
        "#extracted_data.head()\n"
      ],
      "metadata": {
        "id": "Zfm4TZJD0PL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "### YOUR CODE HERE ###\n",
        "df0.isna().sum()\n",
        "\n",
        "# Drop rows with missing values when needed\n",
        "# Save DataFrame in variable `df0_subset`.\n",
        "#df0_subset = df0.dropna(axis=0).reset_index(drop = True)"
      ],
      "metadata": {
        "id": "7mJnQ1Oi1bF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicates\n",
        "### YOUR CODE HERE ###\n",
        "df0.duplicated().sum()"
      ],
      "metadata": {
        "id": "N33nWSoH1mwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect some rows containing duplicates as needed\n",
        "### YOUR CODE HERE ###\n",
        "df0[df0.duplicated()].head()"
      ],
      "metadata": {
        "id": "mjKTQLV010IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a boxplot to visualize distribution of `tenure` and detect any outliers\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.title('Boxplot to detect outliers for tenure', fontsize=12)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "sns.boxplot(x=df1['tenure'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zJvpeYUM2Lk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of rows containing outliers \n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Compute the 25th percentile value in `tenure`\n",
        "percentile25 = df1['tenure'].quantile(0.25)\n",
        "\n",
        "# Compute the 75th percentile value in `tenure`\n",
        "percentile75 = df1['tenure'].quantile(0.75)\n",
        "\n",
        "# Compute the interquartile range in `tenure`\n",
        "iqr = percentile75 - percentile25\n",
        "\n",
        "# Define the upper limit and lower limit for non-outlier values in `tenure`\n",
        "upper_limit = percentile75 + 1.5 * iqr\n",
        "lower_limit = percentile25 - 1.5 * iqr\n",
        "print(\"Lower limit:\", lower_limit)\n",
        "print(\"Upper limit:\", upper_limit)\n",
        "\n",
        "# Identify subset of data containing outliers in `tenure`\n",
        "outliers = df1[(df1['tenure'] > upper_limit) | (df1['tenure'] < lower_limit)]\n",
        "\n",
        "# Count how many rows in the data contain outliers in `tenure`\n",
        "print(\"Number of rows in the data containing outliers in `tenure`:\", len(outliers))"
      ],
      "metadata": {
        "id": "Qf8MSFHk2ahW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pAce"
      ],
      "metadata": {
        "id": "Y_9suzgo6mEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the distribution of number_project for those who stayed and those who left.\n",
        "# Create a plot as needed \n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Set figure and axes\n",
        "fig, ax = plt.subplots(1, 2, figsize = (22,8))\n",
        "\n",
        "# Create boxplot showing `average_monthly_hours` distributions for `number_project`, comparing employees who stayed versus those who left\n",
        "sns.boxplot(data=df1, x='average_monthly_hours', y='number_project', hue='left', orient=\"h\", ax=ax[0])\n",
        "ax[0].invert_yaxis()\n",
        "ax[0].set_title('Monthly hours by number of projects', fontsize='14')\n",
        "\n",
        "# Create histogram showing distribution of `number_project`, comparing employees who stayed versus those who left\n",
        "tenure_stay = df1[df1['left']==0]['number_project']\n",
        "tenure_left = df1[df1['left']==1]['number_project']\n",
        "sns.histplot(data=df1, x='number_project', hue='left', multiple='dodge', shrink=2, ax=ax[1])\n",
        "ax[1].set_title('Number of projects histogram', fontsize='14')\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TDuU7Tlo2cgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get value counts of stayed/left for employees with 7 projects\n",
        "df1[df1['number_project']==7]['left'].value_counts()"
      ],
      "metadata": {
        "id": "kmj-5Jht7nGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Next, you could examine the average monthly hours versus the satisfaction levels\n",
        "# Create a plot as needed \n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Create scatterplot of `average_monthly_hours` versus `satisfaction_level`, comparing employees who stayed versus those who left\n",
        "plt.figure(figsize=(16, 9))\n",
        "sns.scatterplot(data=df1, x='average_monthly_hours', y='satisfaction_level', hue='left', alpha=0.4)\n",
        "plt.axvline(x=166.67, color='#ff6361', label='166.67 hrs./mo.', ls='--')\n",
        "plt.legend(labels=['166.67 hrs./mo.', 'left', 'stayed'])\n",
        "plt.title('Monthly hours by last evaluation score', fontsize='14');"
      ],
      "metadata": {
        "id": "j9L8FnvV7n4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For the next visualization, it might be interesting to visualize satisfaction levels by tenure.\n",
        "# Create a plot as needed \n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Set figure and axes\n",
        "fig, ax = plt.subplots(1, 2, figsize = (22,8))\n",
        "\n",
        "# Create boxplot showing distributions of `satisfaction_level` by tenure, comparing employees who stayed versus those who left\n",
        "sns.boxplot(data=df1, x='satisfaction_level', y='tenure', hue='left', orient=\"h\", ax=ax[0])\n",
        "ax[0].invert_yaxis()\n",
        "ax[0].set_title('Satisfaction by tenure', fontsize='14')\n",
        "\n",
        "# Create histogram showing distribution of `tenure`, comparing employees who stayed versus those who left\n",
        "tenure_stay = df1[df1['left']==0]['tenure']\n",
        "tenure_left = df1[df1['left']==1]['tenure']\n",
        "sns.histplot(data=df1, x='tenure', hue='left', multiple='dodge', shrink=5, ax=ax[1])\n",
        "ax[1].set_title('Tenure histogram', fontsize='14')\n",
        "\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "GzFDBNwR76nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean and median satisfaction scores of employees who left and those who stayed\n",
        "df1.groupby(['left'])['satisfaction_level'].agg([np.mean,np.median])"
      ],
      "metadata": {
        "id": "0SuhgZPD8qfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Next, you could examine salary levels for different tenures\n",
        "# Create a plot as needed \n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Set figure and axes\n",
        "fig, ax = plt.subplots(1, 2, figsize = (22,8))\n",
        "\n",
        "# Define short-tenured employees\n",
        "tenure_short = df1[df1['tenure'] < 7]\n",
        "\n",
        "# Define long-tenured employees\n",
        "tenure_long = df1[df1['tenure'] > 6]\n",
        "\n",
        "# Plot short-tenured histogram\n",
        "sns.histplot(data=tenure_short, x='tenure', hue='salary', discrete=1, \n",
        "             hue_order=['low', 'medium', 'high'], multiple='dodge', shrink=.5, ax=ax[0])\n",
        "ax[0].set_title('Salary histogram by tenure: short-tenured people', fontsize='14')\n",
        "\n",
        "# Plot long-tenured histogram\n",
        "sns.histplot(data=tenure_long, x='tenure', hue='salary', discrete=1, \n",
        "             hue_order=['low', 'medium', 'high'], multiple='dodge', shrink=.4, ax=ax[1])\n",
        "ax[1].set_title('Salary histogram by tenure: long-tenured people', fontsize='14');"
      ],
      "metadata": {
        "id": "GGaFeShA9S11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Next, you could explore whether there's a correlation between working long hours and receiving high evaluation scores. \n",
        "#You could create a scatterplot of average_monthly_hours versus last_evaluation.\n",
        "# Create a plot as needed \n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Create scatterplot of `average_monthly_hours` versus `last_evaluation`\n",
        "plt.figure(figsize=(16, 9))\n",
        "sns.scatterplot(data=df1, x='average_monthly_hours', y='last_evaluation', hue='left', alpha=0.4)\n",
        "plt.axvline(x=166.67, color='#ff6361', label='166.67 hrs./mo.', ls='--')\n",
        "plt.legend(labels=['166.67 hrs./mo.', 'left', 'stayed'])\n",
        "plt.title('Monthly hours by last evaluation score', fontsize='14');"
      ],
      "metadata": {
        "id": "BEDZzqQ99Wta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Next, you could examine whether employees who worked very long hours were promoted in the last five years.\n",
        "# Create a plot as needed \n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Create plot to examine relationship between `average_monthly_hours` and `promotion_last_5years`\n",
        "plt.figure(figsize=(16, 3))\n",
        "sns.scatterplot(data=df1, x='average_monthly_hours', y='promotion_last_5years', hue='left', alpha=0.4)\n",
        "plt.axvline(x=166.67, color='#ff6361', ls='--')\n",
        "plt.legend(labels=['166.67 hrs./mo.', 'left', 'stayed'])\n",
        "plt.title('Monthly hours by promotion last 5 years', fontsize='14');"
      ],
      "metadata": {
        "id": "4hseIRQl95ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Next, you could inspect how the employees who left are distributed across departments.\n",
        "# Display counts for each department\n",
        "df1[\"department\"].value_counts()"
      ],
      "metadata": {
        "id": "kgd-GTwm-98H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a plot as needed \n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Create stacked histogram to compare department distribution of employees who left to that of employees who didn't\n",
        "plt.figure(figsize=(11,8))\n",
        "sns.histplot(data=df1, x='department', hue='left', discrete=1, \n",
        "             hue_order=[0, 1], multiple='dodge', shrink=.5)\n",
        "plt.xticks(rotation='45')\n",
        "plt.title('Counts of stayed/left by department', fontsize=14);"
      ],
      "metadata": {
        "id": "ym2V8W8TB-pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lastly, you could check for strong correlations between variables in the data.\n",
        "# Create a plot as needed \n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Plot a correlation heatmap\n",
        "plt.figure(figsize=(16, 9))\n",
        "heatmap = sns.heatmap(df0.corr(), vmin=-1, vmax=1, annot=True, cmap=sns.color_palette(\"vlag\", as_cmap=True))\n",
        "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':14}, pad=12);"
      ],
      "metadata": {
        "id": "A2AAcMXjCY0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "paCe"
      ],
      "metadata": {
        "id": "gzeJ-3HCC3JA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic regression\n",
        "#Note that binomial logistic regression suits the task because it involves binary classification.\n",
        "\n",
        "#Start by one-hot encoding the categorical variables as needed.\n",
        "###Logistic regression###\n",
        "#Note that binomial logistic regression suits the task because it involves binary classification.\n",
        "#Start by one-hot encoding the categorical variables as needed.\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "# One-hot encode the categorical variables as needed and save resulting dataframe in a new variable\n",
        "df_enc = pd.get_dummies(df1, prefix=['salary', 'dept'], columns = ['salary', 'department'], drop_first=False)\n",
        "\n",
        "# Display the new dataframe\n",
        "df_enc.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8HkYmXe7C5l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a heatmap to visualize how correlated variables are.\n",
        "#Consider which variables you're interested in examining correlations between\n",
        "\n",
        "# Create a heatmap to visualize how correlated variables are\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df_enc[['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours', 'tenure']].corr(), annot=True, cmap=\"crest\")\n",
        "plt.title('Heatmap of the dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xc6PZ79dFY0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a stacked bart plot to visualize number of employees across department, comparing those who left with those who didn't.\n",
        "# Create a stacked bart plot to visualize number of employees across department, comparing those who left with those who didn't\n",
        "# In the legend, 0 (purple color) represents employees who did not leave, 1 (red color) represents employees who left\n",
        "pd.crosstab(df1[\"department\"], df1[\"left\"]).plot(kind ='bar',color='mr')\n",
        "plt.title('Counts of employees who left versus stayed across department')\n",
        "plt.ylabel('Employee count')\n",
        "plt.xlabel('Department')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pxu68zCyFZux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Since logistic regression is quite sensitive to outliers, \n",
        "#it would be a good idea at this stage to remove the outliers in the tenure column that were identified earlier\n",
        "# Select rows without outliers in `tenure` and save resulting dataframe in a new variable\n",
        "df_logreg = df_enc[(df_enc['tenure'] >= lower_limit) & (df_enc['tenure'] <= upper_limit)]\n",
        "\n",
        "# Display first few rows of new dataframe\n",
        "df_logreg.head()"
      ],
      "metadata": {
        "id": "cLYjL_sXF3oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Isolate the outcome variable, which is the variable you want your model to predict.\n",
        "# Isolate the outcome variable\n",
        "y = df_logreg['left']\n",
        "\n",
        "# Display first few rows of the outcome variable\n",
        "y.head() "
      ],
      "metadata": {
        "id": "b8YOgNkTHKXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the features you want to use in your model. Consider which variables will help you predict the outcome variable, left.\n",
        "# Select the features you want to use in your model\n",
        "X = df_logreg[['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours', 'tenure', 'work_accident', 'promotion_last_5years', 'salary_high', 'salary_low', 'salary_medium' , 'dept_IT', 'dept_RandD', 'dept_accounting', 'dept_hr', 'dept_management', 'dept_marketing', 'dept_product_mng', 'dept_sales', 'dept_support', 'dept_technical']]\n",
        "\n",
        "# Display the first few rows of the selected features \n",
        "X.head()"
      ],
      "metadata": {
        "id": "Kd1o8rr5HN1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training set and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7k1C2DC2HQq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a logistic regression model and fit it to the training dataset\n",
        "log_clf = LogisticRegression(random_state=42, max_iter=500).fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "clmjZpBDHlcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the logistic regression model to get predictions on the test set\n",
        "y_pred = log_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "kSU4x8qNH1S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a confusion matrix to visualize the results of the logistic regression model.\n",
        "# Compute values for confusion matrix\n",
        "log_cm = confusion_matrix(y_test, y_pred, labels=log_clf.classes_)\n",
        "\n",
        "# Create display of confusion matrix\n",
        "log_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, display_labels=log_clf.classes_)\n",
        "\n",
        "# Plot confusion matrix\n",
        "log_disp.plot()\n",
        "\n",
        "# Display plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3feCoqBnH9Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create a classification report that includes precision, recall, f1-score, and accuracy metrics \n",
        "# to evaluate the performance of the logistic regression model.\n",
        "\n",
        "# # Check the class balance in the data. In other words, check the value counts in the left column. \n",
        "# Since this is a binary classification task, the class balance informs the way you interpret accuracy metrics.\n",
        "\n",
        "df_logreg['left'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "y2pDj1IuIF_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create classification report for logistic regression model\n",
        "target_names = ['Predicted would not leave', 'Predicted would leave']\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "pTV_FIs_I6fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modeling Approach B: Tree-based Model"
      ],
      "metadata": {
        "id": "Hxnj3sKcJIr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modeling Approach B: Tree-based Model\n",
        "# This approach covers implementation of Decision Tree and Random Forest.\n",
        "\n",
        "# Encode categorical variables\n",
        "df2 = pd.get_dummies(df1)"
      ],
      "metadata": {
        "id": "euM9lss1I8B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Isolate the outcome variable\n",
        "y = df2['left']\n",
        "\n",
        "# Display the first few rows of `y`\n",
        "y.head()\n"
      ],
      "metadata": {
        "id": "3-HQQutIJNwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the features\n",
        "X = df2.drop('left', axis=1)\n",
        "\n",
        "# Display the first few rows of `X`\n",
        "X.head()"
      ],
      "metadata": {
        "id": "NFz4I--5JYZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the data into training, validating, and testing sets.\n",
        "\n",
        "# Create test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
        "\n",
        "# Create train & validate data\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=0)"
      ],
      "metadata": {
        "id": "DYJW8onfJprz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision tree - Round 1\n",
        "# Construct a decision tree model and set up cross-validated grid-search to exhuastively search for the best model parameters.\n",
        "\n",
        "\n",
        "# Instantiate model\n",
        "tree = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "# Assign a dictionary of hyperparameters to search over\n",
        "cv_params = {'max_depth':[4, 6, 8, None],\n",
        "             'min_samples_leaf': [2, 5, 1],\n",
        "             'min_samples_split': [2, 4, 6]\n",
        "             }\n",
        "\n",
        "# Assign a dictionary of scoring metrics to capture\n",
        "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n",
        "\n",
        "# Instantiate GridSearch\n",
        "tree1 = GridSearchCV(tree, cv_params, scoring=scoring, cv=4, refit='roc_auc')"
      ],
      "metadata": {
        "id": "QJlr7gz4J3_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit the decision tree model to the training data.\n",
        "#%%time\n",
        "tree1.fit(X_tr, y_tr)"
      ],
      "metadata": {
        "id": "oVgnBpGlKL0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check best parameters\n",
        "tree1.best_params_"
      ],
      "metadata": {
        "id": "e9RMf0aUKM-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check best AUC score on CV\n",
        "tree1.best_score_"
      ],
      "metadata": {
        "id": "XdKHbxBUKXYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, you can write a function that will help you extract all the scores from the grid search.\n",
        "\n",
        "def make_results(model_name:str, model_object, metric:str):\n",
        "    '''\n",
        "    Arguments:\n",
        "        model_name (string): what you want the model to be called in the output table\n",
        "        model_object: a fit GridSearchCV object\n",
        "        metric (string): precision, recall, f1, accuracy, or auc\n",
        "  \n",
        "    Returns a pandas df with the F1, recall, precision, accuracy, and auc scores\n",
        "    for the model with the best mean 'metric' score across all validation folds.  \n",
        "    '''\n",
        "\n",
        "    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n",
        "    metric_dict = {'auc': 'mean_test_roc_auc',\n",
        "                 'precision': 'mean_test_precision',\n",
        "                 'recall': 'mean_test_recall',\n",
        "                 'f1': 'mean_test_f1',\n",
        "                 'accuracy': 'mean_test_accuracy',\n",
        "                 }\n",
        "\n",
        "    # Get all the results from the CV and put them in a df\n",
        "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
        "\n",
        "    # Isolate the row of the df with the max(metric) score\n",
        "    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n",
        "\n",
        "    # Extract Accuracy, precision, recall, and f1 score from that row\n",
        "    auc = best_estimator_results.mean_test_roc_auc\n",
        "    f1 = best_estimator_results.mean_test_f1\n",
        "    recall = best_estimator_results.mean_test_recall\n",
        "    precision = best_estimator_results.mean_test_precision\n",
        "    accuracy = best_estimator_results.mean_test_accuracy\n",
        "  \n",
        "    # Create table of results\n",
        "    table = pd.DataFrame()\n",
        "    table = table.append({'Model': model_name,\n",
        "                        'AUC': auc,\n",
        "                        'Precision': precision,\n",
        "                        'Recall': recall,\n",
        "                        'F1': f1,\n",
        "                        'Accuracy': accuracy,\n",
        "                        },\n",
        "                        ignore_index=True\n",
        "                       )\n",
        "  \n",
        "    return table"
      ],
      "metadata": {
        "id": "TVUsegPHKZXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all CV scores\n",
        "tree1_cv_results = make_results('decision tree cv', tree1, 'auc')\n",
        "tree1_cv_results"
      ],
      "metadata": {
        "id": "qN_Wd1YzKw_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random forest - Round 1\n",
        "# Construct a random forest model and set up cross-validated grid-search to exhuastively search for the best model parameters.\n",
        "\n",
        "# Instantiate model\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Assign a dictionary of hyperparameters to search over\n",
        "cv_params = {'max_depth': [3,5, None], \n",
        "             'max_features': [1.0],\n",
        "             'max_samples': [0.7, 1.0],\n",
        "             'min_samples_leaf': [1,2,3],\n",
        "             'min_samples_split': [2,3,4],\n",
        "             'n_estimators': [300, 500],\n",
        "             }  \n",
        "\n",
        "# Assign a dictionary of scoring metrics to capture\n",
        "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n",
        "\n",
        "# Instantiate GridSearch\n",
        "rf1 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='roc_auc')"
      ],
      "metadata": {
        "id": "hNkFfo2WL9E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit the random forest model to the training data.\n",
        "\n",
        "#%%time\n",
        "rf1.fit(X_tr, y_tr) # --> Wall time: ~22min"
      ],
      "metadata": {
        "id": "VFxDDpXNL-Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "cwd = os.getcwd()  # Get the current working directory (cwd)\n",
        "files = os.listdir(cwd)  # Get all the files in that directory\n",
        "print(\"Files in %r: %s\" % (cwd, files))"
      ],
      "metadata": {
        "id": "hssIKjDKY9QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a path to the folder where you want to save the model\n",
        "path = 'c:/models'"
      ],
      "metadata": {
        "id": "Zb6bxHcIMjea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define functions to pickle the model and read in the model.\n",
        "\n",
        "def write_pickle(path, model_object, save_as:str):\n",
        "    '''\n",
        "    In: \n",
        "        path:         path of folder where you want to save the pickle\n",
        "        model_object: a model you want to pickle\n",
        "        save_as:      filename for how you want to save the model\n",
        "\n",
        "    Out: A call to pickle the model in the folder indicated\n",
        "    '''    \n",
        "\n",
        "    with open(path + save_as + '.pickle', 'wb') as to_write:\n",
        "        pickle.dump(model_object, to_write)"
      ],
      "metadata": {
        "id": "kXIj2h9hMmyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pickle(path, saved_model_name:str):\n",
        "    '''\n",
        "    In: \n",
        "        path:             path to folder where you want to read from\n",
        "        saved_model_name: filename of pickled model you want to read in\n",
        "\n",
        "    Out: \n",
        "        model: the pickled model \n",
        "    '''\n",
        "    with open(path + saved_model_name + '.pickle', 'rb') as to_read:\n",
        "        model = pickle.load(to_read)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "xoeLMYQhMzW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the functions defined above to save the model in a pickle file and then read it in.\n",
        "\n",
        "# Write pickle\n",
        "write_pickle(path, rf1, 'hr_rf1')"
      ],
      "metadata": {
        "id": "DZiAQQV6NCXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read pickle\n",
        "rf1 = read_pickle(path, 'hr_rf1')"
      ],
      "metadata": {
        "id": "ReVS_QnENE0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Identify the best AUC score achieved by the random forest model on the training set.\n",
        "\n",
        "# Check best AUC score on CV\n",
        "rf1.best_score_"
      ],
      "metadata": {
        "id": "GLqHdraoNOuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check best params\n",
        "rf1.best_params_"
      ],
      "metadata": {
        "id": "o4-4fq9qNPfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Collect the evaluation scores on the training set for the decision tree and random forest models.\n",
        "\n",
        "# Get all CV scores\n",
        "rf1_cv_results = make_results('random forest cv', rf1, 'auc')\n",
        "print(tree1_cv_results)\n",
        "print(rf1_cv_results)"
      ],
      "metadata": {
        "id": "Aho_i2FdYl6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a function that gets all the scores from a model's predictions.\n",
        "\n",
        "def get_scores(model_name:str, model, X_test_data, y_test_data):\n",
        "    '''\n",
        "    Generate a table of test scores.\n",
        "\n",
        "    In: \n",
        "        model_name (string):  How you want your model to be named in the output table\n",
        "        model:                A fit GridSearchCV object\n",
        "        X_test_data:          numpy array of X_test data\n",
        "        y_test_data:          numpy array of y_test data\n",
        "\n",
        "    Out: pandas df of precision, recall, f1, accuracy, and AUC scores for your model\n",
        "    '''\n",
        "\n",
        "    preds = model.best_estimator_.predict(X_test_data)\n",
        "\n",
        "    auc = round(roc_auc_score(y_test_data, preds), 3)\n",
        "    accuracy = round(accuracy_score(y_test_data, preds), 3)\n",
        "    precision = round(precision_score(y_test_data, preds), 3)\n",
        "    recall = round(recall_score(y_test_data, preds), 3)\n",
        "    f1 = round(f1_score(y_test_data, preds), 3)\n",
        "\n",
        "    table = pd.DataFrame({'model': [model_name],\n",
        "                        'AUC': [auc],\n",
        "                        'precision': [precision], \n",
        "                        'recall': [recall],\n",
        "                        'f1': [f1],\n",
        "                        'accuracy': [accuracy]\n",
        "                        })\n",
        "  \n",
        "    return table"
      ],
      "metadata": {
        "id": "GC-AoAeZYon5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function defined above to get scores for the decision tree model and the random forest model.\n",
        "# Get the results on validation set for both models\n",
        "tree1_val_results = get_scores('decision tree1 val', tree1, X_val, y_val)\n",
        "rf1_val_results = get_scores('random forest1 val', rf1, X_val, y_val)\n",
        "\n",
        "# Concatenate validation scores into table\n",
        "all_val_results1 = [tree1_val_results, rf1_val_results]\n",
        "all_val_results1 = pd.concat(all_val_results1).sort_values(by='AUC', ascending=False)\n",
        "all_val_results1\n"
      ],
      "metadata": {
        "id": "NCGmZYUgYsJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions on test data\n",
        "rf1_test_scores = get_scores('random forest1 test', rf1, X_test, y_test)\n",
        "rf1_test_scores"
      ],
      "metadata": {
        "id": "xqPXFQydZeFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "# You might be skeptical of the high evaluation scores. \n",
        "# There is a chance that there is some data leakage occurring. \n",
        "# Data leakage is when you use data to train your model that should not be used during training,\n",
        "# either because it appears in the test data or because it's not data that you'd \n",
        "# expect to have when the model is actually deployed. Training a model with leaked data \n",
        "# can give an unrealistic score that is not replicated in production.\n",
        "\n",
        "# In this case, it's likely that the company won't have satisfaction levels reported for all of its employees. \n",
        "# It's also possible that the average_monthly_hours column is a source of some data leakage. \n",
        "# If employees have already decided upon quitting, or have already identified by management \n",
        "# as people to be fired, they may be working fewer hours.\n",
        "\n",
        "# The first round of decision tree and random forest models included all variables as features.\n",
        "# This next round will incorporate feature engineering to build improved models.\n",
        "\n",
        "# You could proceed by dropping satisfaction_level and creating a new feature that roughly \n",
        "# captures whether an employee is overworked. You could call this new feature overworked. \n",
        "# It will be a binary variable.\n",
        "\n",
        "# Drop `satisfaction_level` and save resulting dataframe in new variable\n",
        "df3 = df1.drop('satisfaction_level', axis=1)\n",
        "\n",
        "# Display first few rows of new dataframe\n",
        "df3.head()"
      ],
      "metadata": {
        "id": "NZxGVF5HZi5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create `overworked` column. For now, it's identical to average monthly hours.\n",
        "df3['overworked'] = df3['average_monthly_hours']\n",
        "\n",
        "# Inspect max and min average monthly hours values\n",
        "print('Max hours:', df3['overworked'].max())\n",
        "print('Min hours:', df3['overworked'].min())"
      ],
      "metadata": {
        "id": "vXT6Krppabwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define `overworked` as working > 175 hrs/week\n",
        "df3['overworked'] = (df3['overworked'] > 175).astype(int)\n",
        "\n",
        "# Display first few rows of new column\n",
        "df3['overworked'].head()"
      ],
      "metadata": {
        "id": "iclUla37agDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the `average_monthly_hours` column\n",
        "df3 = df3.drop('average_monthly_hours', axis=1)\n",
        "\n",
        "# Display first few rows of resulting dataframe\n",
        "df3.head()"
      ],
      "metadata": {
        "id": "vxL8tVZWaqEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the categorical variables as needed and save resulting dataframe in a new variable\n",
        "df4 = pd.get_dummies(df3)\n",
        "\n",
        "# Display the new dataframe\n",
        "df4.head()"
      ],
      "metadata": {
        "id": "XZcKePk_arSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Isolate the outcome variable\n",
        "y = df4['left']\n",
        "\n",
        "# Display the first few rows of `y`\n",
        "y.head()"
      ],
      "metadata": {
        "id": "uC_VvOwCa2SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the features\n",
        "X = df4.drop('left', axis=1)\n",
        "\n",
        "# Display the first few rows of `X`\n",
        "X.head()"
      ],
      "metadata": {
        "id": "xKgFWh7Ua4Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
        "\n",
        "# Create train & validate data\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=0)"
      ],
      "metadata": {
        "id": "WqAgg0r9a-PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision tree - Round 2\n",
        "\n",
        "# Instantiate model\n",
        "tree = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "# Assign a dictionary of hyperparameters to search over\n",
        "cv_params = {'max_depth':[4, 6, 8, None],\n",
        "             'min_samples_leaf': [2, 5, 1],\n",
        "             'min_samples_split': [2, 4, 6]\n",
        "             }\n",
        "\n",
        "# Assign a dictionary of scoring metrics to capture\n",
        "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n",
        "\n",
        "# Instantiate GridSearch\n",
        "tree2 = GridSearchCV(tree, cv_params, scoring=scoring, cv=4, refit='roc_auc')"
      ],
      "metadata": {
        "id": "HurP3GMzbL-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tree2.fit(X_tr, y_tr)"
      ],
      "metadata": {
        "id": "yJM_LE4bbM6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check best params\n",
        "tree2.best_params_"
      ],
      "metadata": {
        "id": "IHrcCBMKbVyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check best AUC score on CV\n",
        "tree2.best_score_"
      ],
      "metadata": {
        "id": "Fix8mZjUbWvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all CV scores\n",
        "tree2_cv_results = make_results('decision tree2 cv', tree2, 'auc')\n",
        "tree2_cv_results"
      ],
      "metadata": {
        "id": "eV6KQYUfbjky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random forest - Round 2\n",
        "# Instantiate model\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Assign a dictionary of hyperparameters to search over\n",
        "cv_params = {'max_depth': [3,5, None], \n",
        "             'max_features': [1.0],\n",
        "             'max_samples': [0.7, 1.0],\n",
        "             'min_samples_leaf': [1,2,3],\n",
        "             'min_samples_split': [2,3,4],\n",
        "             'n_estimators': [300, 500],\n",
        "             }  \n",
        "\n",
        "# Assign a dictionary of scoring metrics to capture\n",
        "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n",
        "\n",
        "# Instantiate GridSearch\n",
        "rf2 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='roc_auc')"
      ],
      "metadata": {
        "id": "n3elP_JBb1Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rf2.fit(X_tr, y_tr) # --> Wall time: 17min 5s"
      ],
      "metadata": {
        "id": "3Shy5Gg8b2cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write pickle\n",
        "write_pickle(path, rf2, 'hr_rf2')"
      ],
      "metadata": {
        "id": "h6EKMCfJb8tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in pickle\n",
        "rf2 = read_pickle(path, 'hr_rf2')"
      ],
      "metadata": {
        "id": "qfpU3jDkb_iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check best params\n",
        "rf2.best_params_"
      ],
      "metadata": {
        "id": "7gXf_KfgcHeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check best AUC score on CV\n",
        "rf2.best_score_"
      ],
      "metadata": {
        "id": "JcqE4-eFcKaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all CV scores\n",
        "rf2_cv_results = make_results('random forest2 cv', rf2, 'auc')\n",
        "print(tree2_cv_results)\n",
        "print(rf2_cv_results)"
      ],
      "metadata": {
        "id": "6hu-3H68cQYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect validation scores \n",
        "tree2_val_results = get_scores('decision tree2 val', tree2, X_val, y_val)\n",
        "rf2_val_results = get_scores('random forest2 val', rf2, X_val, y_val)\n",
        "\n",
        "# Concatenate validation scores into table\n",
        "all_val_results2 = [tree2_val_results, rf2_val_results]\n",
        "all_val_results2 = pd.concat(all_val_results2).sort_values(by='AUC', ascending=False)\n",
        "all_val_results2"
      ],
      "metadata": {
        "id": "9O6NfLrdcSzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions on test data\n",
        "rf2_test_scores = get_scores('random forest2 test', rf2, X_test, y_test)\n",
        "rf2_test_scores"
      ],
      "metadata": {
        "id": "AeYHPqajcXs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate array of values for confusion matrix\n",
        "preds = rf2.best_estimator_.predict(X_test)\n",
        "cm = confusion_matrix(y_test, preds, labels=rf2.classes_)\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                             display_labels=rf2.classes_)\n",
        "disp.plot();"
      ],
      "metadata": {
        "id": "xVu6xex4ccvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision tree splits\n",
        "\n",
        "# Plot the tree\n",
        "plt.figure(figsize=(85,20))\n",
        "plot_tree(tree2.best_estimator_, max_depth=6, fontsize=14, feature_names=X.columns, \n",
        "          class_names={0:'stayed', 1:'left'}, filled=True);\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XFBo5Gl8cdkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tree2_importances = pd.DataFrame(tree2.best_estimator_.feature_importances_, columns=X.columns)\n",
        "tree2_importances = pd.DataFrame(tree2.best_estimator_.feature_importances_, columns=['gini_importance'], index=X.columns)\n",
        "tree2_importances = tree2_importances.sort_values(by='gini_importance', ascending=False)\n",
        "\n",
        "# Only extract the features with importances > 0\n",
        "tree2_importances = tree2_importances[tree2_importances['gini_importance'] != 0]\n",
        "tree2_importances"
      ],
      "metadata": {
        "id": "AyfEwsAlc6M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=tree2_importances, x=\"gini_importance\", y=tree2_importances.index, orient='h')\n",
        "plt.title(\"Decision Tree: Feature Importances for Employee Leaving\", fontsize=12)\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_VlAGAc4c8bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random forest feature importance\n",
        "\n",
        "# Get feature importances\n",
        "feat_impt = rf2.best_estimator_.feature_importances_\n",
        "\n",
        "# Get indices of top 10 features\n",
        "ind = np.argpartition(rf2.best_estimator_.feature_importances_, -10)[-10:]\n",
        "\n",
        "# Get column labels of top 10 features \n",
        "feat = X.columns[ind]\n",
        "\n",
        "# Filter `feat_impt` to consist of top 10 feature importances\n",
        "feat_impt = feat_impt[ind]\n",
        "\n",
        "y_df = pd.DataFrame({\"Feature\":feat,\"Importance\":feat_impt})\n",
        "y_sort_df = y_df.sort_values(\"Importance\")\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(111)\n",
        "\n",
        "y_sort_df.plot(kind='barh',ax=ax1,x=\"Feature\",y=\"Importance\")\n",
        "\n",
        "ax1.set_title(\"Random Forest: Feature Importances for Employee Leaving\", fontsize=12)\n",
        "ax1.set_ylabel(\"Feature\")\n",
        "ax1.set_xlabel(\"Importance\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4jsibJQ_c-oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7seq8OpHdLNf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}